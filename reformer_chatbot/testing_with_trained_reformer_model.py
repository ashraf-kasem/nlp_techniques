import json
import os
import random
import shutil

import numpy as np
from termcolor import colored
import trax
from trax import layers as tl
from trax.supervised import training


def tokenize(sentence, vocab_file, vocab_dir):
    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]


def detokenize(tokens, vocab_file, vocab_dir):
    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)


# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention
def attention(*args, **kwargs):
    # number of input positions to remember in a cache when doing fast inference.
    kwargs['predict_mem_len'] = 120
    # number of input elements to drop once the fast inference input cache fills up.
    kwargs['predict_drop_len'] = 120
    # return the attention layer with the parameters defined above
    return tl.SelfAttention(*args, **kwargs)


# function build a Reformer model
def ReformerLM(vocab_size=33000,
               n_layers=2,
               mode='train',
               attention_type=tl.SelfAttention):
    # initialize an instance of Trax's ReformerLM class
    model = tl.Serial(
        trax.models.reformer.ReformerLM(
            # set vocab size
            vocab_size=vocab_size,
            # set number of layers
            n_layers=n_layers,
            # set mode
            mode=mode,
            # set attention type
            attention_type=attention_type
        )
        , tl.LogSoftmax()
    )
    return model


def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature, tokenize=tokenize):
    """
    Args:
        ReformerLM:  the Reformer language model you just trained
        start_sentence (string): starting sentence of the conversation
        vocab_file (string): vocabulary filename
        vocab_dir (string): directory of the vocabulary file
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)

    Returns:
        generator: yields the next symbol generated by the model
    """
    # Create input tokens using the the tokenize function
    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)
    # Add batch dimension to array. Convert from (n,) to (1, n)
    input_tokens_with_batch = np.reshape(np.array(input_tokens), [1, -1])
    # call the autoregressive_sample_stream function from trax
    output_gen = trax.supervised.decoding.autoregressive_sample_stream(ReformerLM, inputs=input_tokens_with_batch,
                                                                       temperature=temperature)
    return output_gen


def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file,
                      vocab_dir, max_len, temperature):
    """
    Args:
        ReformerLM:  the Reformer language model you just trained
        model_state (np.array): initial state of the model before decoding
        start_sentence (string): starting sentence of the conversation
        vocab_file (string): vocabulary filename
        vocab_dir (string): directory of the vocabulary file
        max_len (int): maximum number of tokens to generate
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)

    Returns:
        generator: yields the next symbol generated by the model
    """

    # define the delimiters we used during training
    delimiter_1 = 'Person 1: '
    delimiter_2 = 'Person 2: '
    # initialize detokenized output
    sentence = ''
    # token counter
    counter = 0
    # output tokens. we insert a ': ' for formatting
    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]
    # reset the model state when starting a new dialogue
    ReformerLM.state = model_state
    # calls the output generator implemented earlier
    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR,
                                   temperature=temperature)
    # print the starting sentence
    print(start_sentence.split(delimiter_2)[0].strip())
    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.
    for o in output:
        result.append(o)
        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)
        if sentence.endswith(delimiter_1):
            sentence = sentence.split(delimiter_1)[0]
            print(f'{delimiter_2}{sentence}')
            sentence = ''
            result.clear()
        elif sentence.endswith(delimiter_2):
            sentence = sentence.split(delimiter_2)[0]
            print(f'{delimiter_1}{sentence}')
            sentence = ''
            result.clear()
        counter += 1
        if counter > max_len:
            break


# build by me to make the dialog chat interactve
def interactive_chat(ReformerLM, model_state, vocab_file,
                     vocab_dir, max_len, temperature):
    # define the delimiters we used during training
    delimiter_1 = ' Person 1: '
    delimiter_2 = ' Person 2: '
    # output tokens. we insert a ': ' for formatting
    result = []
    # # reset the model state when starting a new dialogue
    # ReformerLM.state = model_state

    # initialize detokenized output
    current_sentences = ''

    print("chat started, if you want to exit type -1")
    while True:

        # ask for user input
        user_input = input("You: ")
        if user_input == "-1":
            break

        start_sentence = delimiter_1 + user_input + delimiter_2
        # add the new sentence to the previous context
        current_sentences += start_sentence


        # print(" current sentences: ", current_sentences)
        # print()

        # calls the output generator implemented earlier
        # reset the model state when starting a new dialogue
        ReformerLM.state = model_state
        output = ReformerLM_output_gen(ReformerLM, current_sentences, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR,
                                       temperature=temperature)
        # print the starting sentence
        # print(start_sentence.split(delimiter_2)[0].strip())
        # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.
        for o in output:
            result.append(o)
            sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)
            if sentence.endswith(delimiter_1):
                sentence = sentence.split(delimiter_1)[0]
                # we should add the new sentence to the previous context
                current_sentences += sentence
                sentence = sentence.replace(delimiter_2, "")
                print(f'{delimiter_2}{sentence}')
                sentence = ''
                result.clear()
                break



if __name__ == "__main__":
    # vocabulary filename
    VOCAB_FILE = 'en_32k.subword'
    # vocabulary file directory
    VOCAB_DIR = 'data/vocabs'
    # define the model using the ReformerLM function you implemented earlier.
    model = ReformerLM(
        vocab_size=33000,
        n_layers=6,
        mode='predict',
        attention_type=attention,
    )
    # define an input signature so we can initialize our model.
    # shape will be (1, 1) and the data type is int32.
    shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)
    # initialize from file
    model.init_from_file('chatbot_model1.pkl.gz',
                         weights_only=True, input_signature=shape11)
    # save the starting state
    STARTING_STATE = model.state

    # # decoding ex1

    sample_sentence = ' Person 1: Are there theatres in town? Person 2: '
    print("example number 1, start sentence: {}".format(sample_sentence))
    print()
    generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence,
                      vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.5)
    # ex2
    print()
    sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '
    print("example number 2, start sentence: {}".format(sample_sentence))
    print()
    generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence,
                      vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)
    # ex3
    print()
    sample_sentence = ' Person 1: Can you book a taxi? Person 2: '
    print("example number 3, start sentence: {}".format(sample_sentence))
    print()
    generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence,
                      vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)

    # interactive chat
    print()
    print("interactive chat:")
    interactive_chat(ReformerLM=model, model_state=STARTING_STATE,
                     vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)
